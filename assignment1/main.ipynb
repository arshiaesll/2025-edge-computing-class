{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Convolutional Neural Networks (CNNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/assignment1/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/assignment1/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /opt/homebrew/Caskroom/miniforge/base/envs/assignment1/lib/python3.12/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/homebrew/Caskroom/miniforge/base/envs/assignment1/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/assignment1/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/assignment1/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/assignment1/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Import torch packages\n",
    "import torch\n",
    "import torchvision as torchv\n",
    "\n",
    "# Packages that are nice to have\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from torchsummary import summary\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./dataset/train/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/train/MNIST/raw/train-images-idx3-ubyte.gz to ./dataset/train/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./dataset/train/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/train/MNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/train/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./dataset/train/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/train/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/train/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./dataset/train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/train/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/train/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./dataset/test/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/test/MNIST/raw/train-images-idx3-ubyte.gz to ./dataset/test/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./dataset/test/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/test/MNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/test/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./dataset/test/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/test/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/test/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./dataset/test/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/test/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/test/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchv.datasets.MNIST(\n",
    "    root='./dataset/train',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchv.transforms.ToTensor()\n",
    ")\n",
    "test_dataset = torchv.datasets.MNIST(\n",
    "    root='./dataset/test',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchv.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
      "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
      "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
      "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
      "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
      "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
      "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
      "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
      "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
      "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
      "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
      "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
      "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
      "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]), 5)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(type(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single sample of the data is a tuple of length 2. This will be the input into our convolutional neural network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# First element of tuple. This is an image!\n",
    "print(train_dataset[0][0].shape)\n",
    "print(type(train_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may not be familiar with what a \"tensor\" is as of now. Just think of it as a datatype similar to an array!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a single sample of the second element. This will be our label (or \"target\" in PyTorch) to compare the output of our network to in order to determine the loss or error of our network so we can tell the network how to improve the network.\n",
    "\n",
    "Keep in mind this is can also be called the ground-truth of the input sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Second element is just a single integer!\n",
    "print(train_dataset[0][1])\n",
    "print(type(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are these two elements of the single sample?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first element is an image of shape 1x28x28 (NumberOfChanngels \\* Width \\* Height) representing a single handwritten digit (i.e. a number between 0 and 9).\n",
    "- The second element is a single digit integer representing the handwritten digit's perceived value contained within the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first element of the tuple, we see that the shape of the image is 1x28x28. Why do we have the single 1 in the front? This is the number of color channels the image has in it. In this case, we only have one color channel conveying that this image is simply a gray scale image!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q1: How many color channels does a traditional colored image have?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that this is only ONE sample of the dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 samples in the training dataset!\n",
      "There are 10000 samples in the test dataset!\n",
      "Thus we have 70000 samples in total!\n",
      "That is, we have a total of 70000 tuples containing a single grayscale image and a single digit integer!\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(train_dataset)} samples in the training dataset!')\n",
    "print(f'There are {len(test_dataset)} samples in the test dataset!')\n",
    "print(\n",
    "    f'Thus we have {len(train_dataset) + len(test_dataset)} ' +\n",
    "    'samples in total!\\n' +\n",
    "    f'That is, we have a total of {len(train_dataset) + len(test_dataset)} ' +\n",
    "    'tuples containing a single grayscale image and a single digit integer!'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and Split the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently have two separate parts of the dataset: the training data and the testing data. For this example we will combine these two into one large dataset of 70,000 samples and then split the dataset into three new datasets: train, validation, and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images: (60000, 28, 28)\n",
      "Train Labels: (60000,)\n",
      "Test Images: (10000, 28, 28)\n",
      "Test Labels: (10000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the data into numpy arrays\n",
    "# (convert data to numpy format to manipulate later)\n",
    "train_images = train_dataset.data.numpy()\n",
    "train_labels = train_dataset.targets.numpy()\n",
    "test_images = test_dataset.data.numpy()\n",
    "test_labels = test_dataset.targets.numpy()\n",
    "print(\n",
    "    f'Train Images: {train_images.shape}\\n' +\n",
    "    f'Train Labels: {train_labels.shape}\\n' +\n",
    "    f'Test Images: {test_images.shape}\\n' +\n",
    "    f'Test Labels: {test_labels.shape}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Images: (70000, 28, 28)\n",
      "All Labels: (70000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the train and test dataset into one big dataset\n",
    "all_images = np.concat([train_images, test_images])\n",
    "all_labels = np.concat([train_labels, test_labels])\n",
    "print(\n",
    "    f'All Images: {all_images.shape}\\n' +\n",
    "    f'All Labels: {all_labels.shape}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice! That the color channel dimension of 1 was removed! This is fine for now. We will add it back later!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have one large dataset, we split the data into the proportions we wish to use for our three separate datasets for training, validation, and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 14000, 7000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the large numpy array into smaller train, validation, and test splits\n",
    "choices = np.arange(len(all_labels))\n",
    "# Specify the percentages of samples each set should contain\n",
    "# NOTE: The last percentage is not important here since we will just\n",
    "#       use the remaining images after we take out the training and validation\n",
    "#       sets.\n",
    "train_perc, val_perc, test_perc = (0.7, 0.2, 0.1)\n",
    "# Get the number of total samples\n",
    "num_samples = len(all_labels)\n",
    "# Calculate the number train samples we want\n",
    "num_train = int(np.floor(num_samples * train_perc))\n",
    "# Calculate the number of validation samples we want\n",
    "num_val = int(np.floor(num_samples * val_perc))\n",
    "# Calculate the number of test samples we want\n",
    "num_test = num_samples - num_train - num_val\n",
    "# Show the number of samples in each\n",
    "num_train, num_val, num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11655, 36219, 38164, ...,   821, 31826, 46739], shape=(49000,)),\n",
       " array([30400, 40846, 61377, ..., 63373, 16756, 29256], shape=(14000,)),\n",
       " array([   66,    74,    85, ..., 69971, 69973, 69978], shape=(7000,)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select indices throughout the whole dataset\n",
    "train_idx = np.random.choice(choices, num_train, replace=False)\n",
    "# Get the set difference between the whole dataset and the chosen training\n",
    "# indices\n",
    "choices = np.setdiff1d(choices, train_idx)\n",
    "# Now get randomly choose the validation indices\n",
    "val_idx = np.random.choice(choices, num_val, replace=False)\n",
    "# Similarly, get the set difference but this time the resulting difference\n",
    "# is, in fact, the test set.\n",
    "test_idx = np.setdiff1d(choices, val_idx)\n",
    "# Show index sets\n",
    "train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure disjoint sets i.e. none of the elements overlap between the new\n",
    "# three datasets of train, validation, and test.\n",
    "np.intersect1d(np.intersect1d(train_idx, val_idx), test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we actually get all of the images and labels corresponding to their\n",
    "# sets using the indices we randomly chose\n",
    "train_images, train_labels = all_images[train_idx], all_labels[train_idx]\n",
    "val_images, val_labels = all_images[val_idx], all_labels[val_idx]\n",
    "test_images, test_labels = all_images[test_idx], all_labels[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions and Custom Dataset Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image):\n",
    "    \"\"\"Perform a transform on an input image. This can include normalization,\n",
    "    padding, and other transformations/augmentations.\n",
    "\n",
    "    :param image: The input image.\n",
    "    :type image: Typically a numpy array.\n",
    "    :return: A new transformed image.\n",
    "    :rtype: _type_\n",
    "    \"\"\"\n",
    "    x = np.pad(image, pad_width=2)\n",
    "    x = np.reshape(x, (1, 32, 32))\n",
    "    x = torch.Tensor(x / 255.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def target_transform(label):\n",
    "    \"\"\"Perform transformations on the label (i.e. \"target\").\n",
    "\n",
    "    :param label: An input integer in this case\n",
    "    :type label: A number that we should typecast to integer.\n",
    "    :return: A transformed label.\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    x = int(label)\n",
    "    return x\n",
    "\n",
    "\n",
    "class MNIST_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Create a custom PyTorch dataset with all of the necessary functions to\n",
    "    properly work with other PyTorch operations/functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        images,\n",
    "        labels,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform\n",
    "    ):\n",
    "        \"\"\"The constructor for the class. Initialize variables.\n",
    "\n",
    "        :param images: A group of images.\n",
    "        :param labels: A group of labels.\n",
    "        :param transform: Transform function to apply to images, \n",
    "            defaults to transform\n",
    "        :type transform: function, optional\n",
    "        :param target_transform: Transform function to apply to the labels,\n",
    "            defaults to target_transform\n",
    "        :type target_transform: function, optional\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the length of the dataset.\n",
    "\n",
    "        :return: The length of the labels i.e. the number of elements in the\n",
    "            dataset.\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single element of the dataset via its index and perform the\n",
    "        necessary transforms upon it.\n",
    "\n",
    "        :param idx: The index of the element to retrieve.\n",
    "        :type idx: int\n",
    "        :return: A tuple grouping the image and the label.\n",
    "        :rtype: tuple\n",
    "        \"\"\"\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our custom dataset definition class to wrap our dataset for use with other\n",
    "# PyTorch tools i.e. the PyTorch dataloader.\n",
    "train_dataset = MNIST_Dataset(\n",
    "    train_images, train_labels, transform=transform, target_transform=target_transform)\n",
    "val_dataset = MNIST_Dataset(\n",
    "    val_images, val_labels, transform=transform, target_transform=target_transform)\n",
    "test_dataset = MNIST_Dataset(\n",
    "    test_images, test_labels, transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pause and analyze our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram showing the frequency of each class in the dataset\n",
    "plt.hist(all_labels, bins=np.arange(11) - 0.5, edgecolor='black')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have mixed distribution of frequencies among the 10 classes of handwritten digits with handwritten 1s having the most samples and the 5s having the least samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q2: What happens if our distribution is very unbalanced meaning we have widely different quantities of classes?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we look at the distribution over the train, validation, and test sets, we can see that the distributions are roughly proportional to what we saw previously when the all of the data was contained within a single large dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x708c68355940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Frequencies of each class in the 3 separate datasets\n",
    "labels = np.unique(train_labels)\n",
    "plt.hist([train_labels, val_labels, test_labels])\n",
    "plt.xticks(labels)\n",
    "plt.legend(['Train', 'Val', 'Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get an idea of what our data actually looks like. Here, we have printed a single sample of each class and displayed their corresponding label above them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying one sample of each class\n",
    "visualization_samples = []\n",
    "plt.subplot(2, 5, 1)\n",
    "for i in range(10):\n",
    "    indices = np.where(train_labels == i)[0]\n",
    "    rand_idx = indices[np.random.randint(0, len(indices) - 1)]\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(np.squeeze(train_images[rand_idx]))\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.title(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases our entire dataset may not fit entirely in memory (i.e. RAM). In our case of MNIST, this typically isn't a problem, but for larger datasets and higher dimensional data (i.e. ImageNet datasets), it is common that we need a way to load our data such that it fits in memory.\n",
    "\n",
    "We can customize the code in the MNIST_Dataset class to allow use to only read data from the disk (Hard Drive or Solid State Drive) when needed. This can be seen in the PyTorch examples of the Dataset and Dataloader documentation/tutorials: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "\n",
    "In this case, we are not concerned with memory limitations, unless the system you are running this tutorial on is extremely limited in memory. Thus we have the entire dataset loaded into memory and can freely manipulate it.\n",
    "\n",
    "In many cases, it is much more efficient to use data batching to batch our data when training to speed up training. Instead of inputting a single image, computing the loss, and then backpropogating the error and adjusting the weights, we can perform the forward pass (i.e. input) several images at once and then compute the loss on those samples together and then backpropogate their error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q3: What do we mean by \"train faster\" or \"more efficiently\" here? That is what is faster or more efficient?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial, we will use a batch size of 32 whilst training\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize the Dataloaders, set the batch size to load BATCH_SIZE\n",
    "# samples at a time and also perform a random shuffle of the samples within\n",
    "# each set of data.\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q4: Why do we shuffle the data even within the randomly chosen samples in each dataset?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate two CNN models both being based off of Yann LeCun et. al.'s LeNet-5 CNN proposed in their publication \"Gradient-Based Learning Applied to Document Recognition\" circa 1998.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 10 different classes of data we want to differentiate between\n",
    "# (i.e. the handwritten single digit number zero thru nine)\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic LeNet-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a close-to-classical version of LeNet-5 which is very similar to what Yann LeCun et. al. proposed in 1998.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OGLeNet5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the layers\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=6,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.avgpool1 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=6,\n",
    "            out_channels=16,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.avgpool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(400, 120)\n",
    "        self.linear2 = torch.nn.Linear(120, 84)\n",
    "        self.linear3 = torch.nn.Linear(84, out_features=NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.functional.F.tanh(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.functional.F.tanh(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = torch.functional.F.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.functional.F.tanh(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Original LeNet-5 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 6, 28, 28]           156\n",
      "├─AvgPool2d: 1-2                         [-1, 6, 14, 14]           --\n",
      "├─Conv2d: 1-3                            [-1, 16, 10, 10]          2,416\n",
      "├─AvgPool2d: 1-4                         [-1, 16, 5, 5]            --\n",
      "├─Flatten: 1-5                           [-1, 400]                 --\n",
      "├─Linear: 1-6                            [-1, 120]                 48,120\n",
      "├─Linear: 1-7                            [-1, 84]                  10,164\n",
      "├─Linear: 1-8                            [-1, 10]                  850\n",
      "==========================================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.42\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.29\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 6, 28, 28]           156\n",
       "├─AvgPool2d: 1-2                         [-1, 6, 14, 14]           --\n",
       "├─Conv2d: 1-3                            [-1, 16, 10, 10]          2,416\n",
       "├─AvgPool2d: 1-4                         [-1, 16, 5, 5]            --\n",
       "├─Flatten: 1-5                           [-1, 400]                 --\n",
       "├─Linear: 1-6                            [-1, 120]                 48,120\n",
       "├─Linear: 1-7                            [-1, 84]                  10,164\n",
       "├─Linear: 1-8                            [-1, 10]                  850\n",
       "==========================================================================================\n",
       "Total params: 61,706\n",
       "Trainable params: 61,706\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.42\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.24\n",
       "Estimated Total Size (MB): 0.29\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_lenet5 = OGLeNet5()\n",
    "summary(original_lenet5, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a More Modern LeNet-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide a more modernize version of LeNet-5 with layers and operations that were not yet available or discovered at the time of LeNet-5's creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernLeNet5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the layers\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=6,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(6)\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=6,\n",
    "            out_channels=16,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(16)\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(400, 120)\n",
    "        self.linear2 = torch.nn.Linear(120, 84)\n",
    "        self.linear3 = torch.nn.Linear(84, out_features=NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Modernized LeNet-5 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 6, 28, 28]           156\n",
      "├─BatchNorm2d: 1-2                       [-1, 6, 28, 28]           12\n",
      "├─MaxPool2d: 1-3                         [-1, 6, 14, 14]           --\n",
      "├─Conv2d: 1-4                            [-1, 16, 10, 10]          2,416\n",
      "├─BatchNorm2d: 1-5                       [-1, 16, 10, 10]          32\n",
      "├─MaxPool2d: 1-6                         [-1, 16, 5, 5]            --\n",
      "├─Flatten: 1-7                           [-1, 400]                 --\n",
      "├─Linear: 1-8                            [-1, 120]                 48,120\n",
      "├─Linear: 1-9                            [-1, 84]                  10,164\n",
      "├─Linear: 1-10                           [-1, 10]                  850\n",
      "==========================================================================================\n",
      "Total params: 61,750\n",
      "Trainable params: 61,750\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.42\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.10\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.34\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 6, 28, 28]           156\n",
       "├─BatchNorm2d: 1-2                       [-1, 6, 28, 28]           12\n",
       "├─MaxPool2d: 1-3                         [-1, 6, 14, 14]           --\n",
       "├─Conv2d: 1-4                            [-1, 16, 10, 10]          2,416\n",
       "├─BatchNorm2d: 1-5                       [-1, 16, 10, 10]          32\n",
       "├─MaxPool2d: 1-6                         [-1, 16, 5, 5]            --\n",
       "├─Flatten: 1-7                           [-1, 400]                 --\n",
       "├─Linear: 1-8                            [-1, 120]                 48,120\n",
       "├─Linear: 1-9                            [-1, 84]                  10,164\n",
       "├─Linear: 1-10                           [-1, 10]                  850\n",
       "==========================================================================================\n",
       "Total params: 61,750\n",
       "Trainable params: 61,750\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.42\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.10\n",
       "Params size (MB): 0.24\n",
       "Estimated Total Size (MB): 0.34\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modern_lenet5 = ModernLeNet5()\n",
    "summary(modern_lenet5, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q5: What are the differences between these two models in their architectures?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Loss Functions and Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paremeters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our learning rate for both models\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our loss function\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Setup an optimizer for each of the two models\n",
    "optim_original_lenet5 = torch.optim.Adam(\n",
    "    original_lenet5.parameters(), lr=LEARNING_RATE)\n",
    "optim_modern_lenet5 = torch.optim.Adam(\n",
    "    modern_lenet5.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of training epochs to loop of the entire dataset\n",
    "NUM_EPOCHS = 10\n",
    "# Determine if we should use\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[1;32m      2\u001b[0m     train_dataloader: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader,\n\u001b[1;32m      3\u001b[0m     model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m      4\u001b[0m     loss_function: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss,\n\u001b[1;32m      5\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam,\n\u001b[1;32m      6\u001b[0m     num_epochs: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      7\u001b[0m     device: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      8\u001b[0m     val_dataloader: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m----> 9\u001b[0m     model_save_path: Path \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m ):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Training loop which iterates over every image in the training dataset,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    performs the forward pass, computes the loss, and then performs the\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    backwards pass.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    # TODO: Implement top-k best model saves\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Initialize lists and variables for storing metrics\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "def train_model(\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    num_epochs: int,\n",
    "    device: str,\n",
    "    val_dataloader: torch.utils.data.DataLoader = None,\n",
    "    model_save_path: Path = Path('./models')\n",
    "):\n",
    "    \"\"\"Training loop which iterates over every image in the training dataset,\n",
    "    performs the forward pass, computes the loss, and then performs the\n",
    "    backwards pass.\n",
    "\n",
    "    See: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "    :param train_dataloader: Dataloader for training data.\n",
    "    :type train_dataloader: torch.utils.data.DataLoader\n",
    "    :param model: The model to train.\n",
    "    :type model: torch.nn.Module\n",
    "    :param loss_function: The loss function to compute the error.\n",
    "    :param optimizer: The optimizer to use during training.\n",
    "    :param num_epochs: Number of epochs to perform training over.\n",
    "    :type num_epochs: int\n",
    "    :param device: Device string to perform the training computations on.\n",
    "    :type device: str\n",
    "    :param val_dataloader: Dataloader for validation data, defaults to None\n",
    "    :type val_dataloader: torch.utils.data.DataLoader, optional\n",
    "    :param model_save_path: Directory where to save the best model to.\n",
    "    :type model_save_path: pathlib.Path\n",
    "\n",
    "    # TODO: Implement top-k best model saves\n",
    "    \"\"\"\n",
    "    # Initialize lists and variables for storing metrics\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_train_acc = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_acc = 0.0\n",
    "    max_train_acc = (0.0, 0)\n",
    "    min_train_loss = (0.0, 0)\n",
    "    max_val_acc = (0.0, 0)\n",
    "    min_val_loss = (0.0, 0)\n",
    "\n",
    "    # Create the save path for the model\n",
    "    if not model_save_path.exists():\n",
    "        model_save_path.mkdir()\n",
    "\n",
    "    # Total number batches in training set\n",
    "    total_steps = len(train_dataloader)\n",
    "\n",
    "    # Perform training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Enable model training\n",
    "        model.train(True)\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_dataloader):\n",
    "            # Move the data to the desired training device\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Perform the forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            # TODO: Add the loss to the running epoch loss\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            # TODO: Compute accuracy metric\n",
    "            # NOTE: Take in as a function for simplicity!\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform the backwards pass (i.e. backpropogate the error)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            # Done with batch, print stats every 500 batches\n",
    "            if (i+1) % 256 == 0:\n",
    "                print(\n",
    "                    f'TRAINING --> Epoch: {epoch}/{num_epochs}, ' +\n",
    "                    f'Step: {i+1}/{total_steps}, ' +\n",
    "                    f'Loss: {loss.item()}'\n",
    "                )\n",
    "\n",
    "        # Print the final stats\n",
    "        print(\n",
    "            f'TRAINING --> Epoch: {epoch}/{num_epochs}, ' +\n",
    "            f'Step: {i+1}/{total_steps}, ' +\n",
    "            f'Loss: {loss.item()}'\n",
    "        )\n",
    "        # End of epoch compute total epoch metrics\n",
    "        epoch_train_loss = epoch_train_loss / (i + 1)\n",
    "        epoch_train_acc = epoch_train_acc / (i + 1)\n",
    "\n",
    "        # Update the best metrics\n",
    "        min_train_loss = (np.min(epoch_train_loss, min_train_loss), epoch)\n",
    "        max_train_acc = (np.max(epoch_train_acc, max_train_acc), epoch)\n",
    "\n",
    "        # Record the accuracy and loss\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        # Now we run over the validation dataset without training\n",
    "        if val_dataloader:\n",
    "            # Disable the gradient calculations and updates\n",
    "            with torch.no_grad():\n",
    "                for i, (images, labels) in enumerate(val_dataloader):\n",
    "                    images.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = loss_function(outputs)\n",
    "\n",
    "        # End of epoch compute total epoch metrics\n",
    "        epoch_val_loss = epoch_val_loss / (i + 1)\n",
    "        epoch_val_acc = epoch_val_acc / (i + 1)\n",
    "\n",
    "        # Update the best metrics\n",
    "        min_val_loss = (np.min(epoch_val_loss, min_val_loss), epoch)\n",
    "        max_val_acc = (np.max(epoch_val_acc, max_val_acc), epoch)\n",
    "\n",
    "        # Record the accuracy and loss\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "\n",
    "        if epoch_val_loss < min_val_loss[0]:\n",
    "            print(\n",
    "                'LOG --> Model with better val_loss of ' +\n",
    "                f'{epoch_val_loss} < {min_val_loss[0]} saving to:\\n' +\n",
    "                f'{model_save_path}'\n",
    "            )\n",
    "            min_val_loss = (epoch_val_loss, epoch)\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "        return (train_losses, train_accs, val_losses, val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q6: What is the purpose of the loss function?_**\n",
    "\n",
    "**_Q7: What is validation doing?_**\n",
    "\n",
    "**_Q8: Why do we perform validation and not just use the training metrics?_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING --> Epoch: 0/10, Step: 500/1532, Loss: 0.3008445203304291\n",
      "TRAINING --> Epoch: 0/10, Step: 1000/1532, Loss: 0.1470094472169876\n",
      "TRAINING --> Epoch: 0/10, Step: 1500/1532, Loss: 0.3648360073566437\n",
      "TRAINING --> Epoch: 1/10, Step: 500/1532, Loss: 0.13600768148899078\n",
      "TRAINING --> Epoch: 1/10, Step: 1000/1532, Loss: 0.16328948736190796\n",
      "TRAINING --> Epoch: 1/10, Step: 1500/1532, Loss: 0.11392730474472046\n",
      "TRAINING --> Epoch: 2/10, Step: 500/1532, Loss: 0.0883319079875946\n",
      "TRAINING --> Epoch: 2/10, Step: 1000/1532, Loss: 0.11061844974756241\n",
      "TRAINING --> Epoch: 2/10, Step: 1500/1532, Loss: 0.02307106927037239\n",
      "TRAINING --> Epoch: 3/10, Step: 500/1532, Loss: 0.02267497405409813\n",
      "TRAINING --> Epoch: 3/10, Step: 1000/1532, Loss: 0.0014593696687370539\n",
      "TRAINING --> Epoch: 3/10, Step: 1500/1532, Loss: 0.06657955795526505\n",
      "TRAINING --> Epoch: 4/10, Step: 500/1532, Loss: 0.0008674622513353825\n",
      "TRAINING --> Epoch: 4/10, Step: 1000/1532, Loss: 0.002181149087846279\n",
      "TRAINING --> Epoch: 4/10, Step: 1500/1532, Loss: 0.007651224732398987\n",
      "TRAINING --> Epoch: 5/10, Step: 500/1532, Loss: 0.003131123259663582\n",
      "TRAINING --> Epoch: 5/10, Step: 1000/1532, Loss: 0.010301691479980946\n",
      "TRAINING --> Epoch: 5/10, Step: 1500/1532, Loss: 0.02668098546564579\n",
      "TRAINING --> Epoch: 6/10, Step: 500/1532, Loss: 0.0015918975695967674\n",
      "TRAINING --> Epoch: 6/10, Step: 1000/1532, Loss: 0.002207110170274973\n",
      "TRAINING --> Epoch: 6/10, Step: 1500/1532, Loss: 0.05678955465555191\n",
      "TRAINING --> Epoch: 7/10, Step: 500/1532, Loss: 0.013942554593086243\n",
      "TRAINING --> Epoch: 7/10, Step: 1000/1532, Loss: 0.014398843050003052\n",
      "TRAINING --> Epoch: 7/10, Step: 1500/1532, Loss: 0.00035197826218791306\n",
      "TRAINING --> Epoch: 8/10, Step: 500/1532, Loss: 0.1152467131614685\n",
      "TRAINING --> Epoch: 8/10, Step: 1000/1532, Loss: 0.0009143943316303194\n",
      "TRAINING --> Epoch: 8/10, Step: 1500/1532, Loss: 0.006976619362831116\n",
      "TRAINING --> Epoch: 9/10, Step: 500/1532, Loss: 0.001552531379275024\n",
      "TRAINING --> Epoch: 9/10, Step: 1000/1532, Loss: 0.013093065470457077\n",
      "TRAINING --> Epoch: 9/10, Step: 1500/1532, Loss: 0.03022228740155697\n"
     ]
    }
   ],
   "source": [
    "# Train the original version of LeNet5\n",
    "original_lenet5.to(DEVICE)\n",
    "train_model(train_dataloader, original_lenet5,\n",
    "            loss, optim_original_lenet5, NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING --> Epoch: 0/10, Step: 500/1532, Loss: 0.05643410608172417\n",
      "TRAINING --> Epoch: 0/10, Step: 1000/1532, Loss: 0.01413065567612648\n",
      "TRAINING --> Epoch: 0/10, Step: 1500/1532, Loss: 0.005406002514064312\n",
      "TRAINING --> Epoch: 1/10, Step: 500/1532, Loss: 0.053792811930179596\n",
      "TRAINING --> Epoch: 1/10, Step: 1000/1532, Loss: 0.018589045852422714\n",
      "TRAINING --> Epoch: 1/10, Step: 1500/1532, Loss: 0.002499380148947239\n",
      "TRAINING --> Epoch: 2/10, Step: 500/1532, Loss: 0.09511638432741165\n",
      "TRAINING --> Epoch: 2/10, Step: 1000/1532, Loss: 0.011492110788822174\n",
      "TRAINING --> Epoch: 2/10, Step: 1500/1532, Loss: 0.10855165868997574\n",
      "TRAINING --> Epoch: 3/10, Step: 500/1532, Loss: 0.025900844484567642\n",
      "TRAINING --> Epoch: 3/10, Step: 1000/1532, Loss: 0.06985034048557281\n",
      "TRAINING --> Epoch: 3/10, Step: 1500/1532, Loss: 0.018113411962985992\n",
      "TRAINING --> Epoch: 4/10, Step: 500/1532, Loss: 0.0018130568787455559\n",
      "TRAINING --> Epoch: 4/10, Step: 1000/1532, Loss: 0.01271965354681015\n",
      "TRAINING --> Epoch: 4/10, Step: 1500/1532, Loss: 0.046061091125011444\n",
      "TRAINING --> Epoch: 5/10, Step: 500/1532, Loss: 0.010048356838524342\n",
      "TRAINING --> Epoch: 5/10, Step: 1000/1532, Loss: 7.693286897847429e-05\n",
      "TRAINING --> Epoch: 5/10, Step: 1500/1532, Loss: 0.03335485979914665\n",
      "TRAINING --> Epoch: 6/10, Step: 500/1532, Loss: 0.000647860171739012\n",
      "TRAINING --> Epoch: 6/10, Step: 1000/1532, Loss: 0.1327996850013733\n",
      "TRAINING --> Epoch: 6/10, Step: 1500/1532, Loss: 0.012494643218815327\n",
      "TRAINING --> Epoch: 7/10, Step: 500/1532, Loss: 0.001184343360364437\n",
      "TRAINING --> Epoch: 7/10, Step: 1000/1532, Loss: 0.03862203285098076\n",
      "TRAINING --> Epoch: 7/10, Step: 1500/1532, Loss: 0.008068040944635868\n",
      "TRAINING --> Epoch: 8/10, Step: 500/1532, Loss: 3.463079337961972e-05\n",
      "TRAINING --> Epoch: 8/10, Step: 1000/1532, Loss: 0.00047616829397156835\n",
      "TRAINING --> Epoch: 8/10, Step: 1500/1532, Loss: 0.022299079224467278\n",
      "TRAINING --> Epoch: 9/10, Step: 500/1532, Loss: 0.009815862402319908\n",
      "TRAINING --> Epoch: 9/10, Step: 1000/1532, Loss: 0.10247983783483505\n",
      "TRAINING --> Epoch: 9/10, Step: 1500/1532, Loss: 0.0009639885392971337\n"
     ]
    }
   ],
   "source": [
    "# Train the modern version of LeNet5\n",
    "modern_lenet5.to(DEVICE)\n",
    "train_model(train_dataloader, modern_lenet5,\n",
    "            loss, optim_modern_lenet5, NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q8: In terms of validation accuracy, which model is the best?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing the Accuracy and Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q9: Is the model under-fitting or over-fitting?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10: How does the test loss and accuracy compare to the best training and validation metrics?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
